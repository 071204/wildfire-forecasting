{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting FWI using deepFWI Pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The Fire Weather Index (FWI) is a meteorologically based index used worldwide to estimate fire danger. It consists of different components that account for the effects of fuel moisture and wind on fire behaviour and spread. The higher the FWI, the more favorable the meteorological conditions to trigger a wildfire. This indicator can help shape long-term tourist strategy and to plan future investments under a changing climate.\n",
    "\n",
    "The [`esowc/wildfire-forecasting`](https://github.com/esowc/wildfire-forecasting) project intends to reproduce the Fire Forecasting capabilities of GEFF using Deep Learning and develop further improvements in accuracy, geography and time scale through inclusion of additional variables or optimisation of model architecture & hyperparameters. \n",
    "\n",
    "In this notebook we demonstrate the use of pre-trained Deep Learning models to perform FWI inference. The Deep Learning Model has been trained on 1 year of data and takes as input the four weather forcings used in the numerical calculation of FWI:\n",
    "```\n",
    "* t2     -    Temperature at 2m (K)\n",
    "* tp     -    Total Precipitation accumulated over the previous 24 hours (mm)\n",
    "* rh     -    Relative Humidity (%)\n",
    "* wspeed -    Windspeed (m/s)\n",
    "```\n",
    "\n",
    "These input weather forcings are supplied for 4 days and the model produces FWI forecast for 10 days. There is 1 day of overlap between the 4 weather forcing variable inputs and the output FWI forecast as indicated below:\n",
    "```\n",
    "                                       ____________\n",
    "Weather Forcings (t-3, t-2, t-1, t) -> | deepFWI  |  -> deepFWI-Forecast (t, t+1, t+2, ..., t+13)\n",
    "                                       ------------\n",
    "```\n",
    "\n",
    "Along with FWI inference using deepGEFF pre-trained models, we also benchmark these results against the baseline `FWI-Forecast` which are numerically calculated FWI predictions from weather forecast.\n",
    "\n",
    "To run this notebook, make sure you have the `wildfire-dl` `conda` environment activated. Instructions for setting up the environment are available in the project [`README`](https://github.com/esowc/wildfire-forecasting/blob/master/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "We first obtain a sample dataset. This is the minimum data that we need to perform inference and then benchmark the results. This data is stored on a publicly accessible Google Cloud Platform (GCP) storage bucket at `gs://deepgeff-data-v0/` and is downloaded into `sampledata/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gs://deepfwi-mini-sample` dataset contains the following data - \n",
    "* `fwi-forcings/`: Weather Forcings for 2019-12-01 to 2019-12-04 --> 4 days --> 4 files \n",
    "* `fwi-forecast/`: Numerical FWI Predictions for 2019-12-04 to 2019-12-13 --> 10 days --> 1 file\n",
    "* `fwi-reanalysis/`: ERA Reanalysis of FWI for 2019-12-04 to 2019-12-13 --> 10 days --> 10 files\n",
    "\n",
    "\n",
    "If you wish to explore these datasets further, have a look at the EDA notebooks in `data/EDA/` in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download a minimal sample dataset consisting of Weather Forcings\n",
    "# for 2019-12-01 to 2019-12-04 --> 4 days --> 4 files\n",
    "!mkdir sampledata/4_14/forcings -p\n",
    "!gsutil -m -q cp -r 'gs://deepgeff-data-v0/forcings/ECMWF_FO_2019120[1-4]*' sampledata/4_14/forcings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download a minimal sample dataset consisting of ERA Reanalysis \n",
    "# for 2019-12-04 to 2019-12-17 --> 14 days --> 14 files\n",
    "!mkdir sampledata/4_14/fwi-reanalysis -p\n",
    "!gsutil -m -q cp -r 'gs://deepgeff-data-v0/fwi-reanalysis/ECMWF_FWI_201912[0][4-9]*' sampledata/4_14/fwi-reanalysis\n",
    "!gsutil -m -q cp -r 'gs://deepgeff-data-v0/fwi-reanalysis/ECMWF_FWI_201912[1][0-7]*' sampledata/4_14/fwi-reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download a minimal sample dataset consisting of Numerical FWI Predictions\n",
    "# for 2019-12-04 to 2019-12-13 --> 10 days --> 1 file\n",
    "!mkdir sampledata/4_14/fwi-forecast -p\n",
    "!gsutil -q cp 'gs://deepgeff-data-v0/fwi-forecast/ECMWF_FWI_20191204_1200_hr_fwi.nc' sampledata/4_14/fwi-forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the pre-trained model\n",
    "For this example notebook, we use the following model-\n",
    "* Run ID: 9x14d9sv that has been trained for 100 epochs on 12 months of T2, TP, RH & WSpeed data for April 2019 - March, 2020; with Undersampling and Class-Balanced loss.\n",
    "\n",
    "If you wish to explore other models, have a look at [pre-trained_models.md](https://github.com/esowc/wildfire-forecasting/blob/master/src/model/checkpoints/pre-trained_models.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download a sample pre-trained model checkpoint file\n",
    "!mkdir samplemodel/4_14/ -p\n",
    "!gsutil -q cp 'gs://deepgeff-models-v0/pre-trained_models/4_14/1ga5loax/1ga5loax_59_60.ckpt' samplemodel/4_14/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the inference and the benchmark\n",
    "\n",
    "For performing inference and then benchmarking the results, we use the `run(**kwargs)` function from [`src/test.py`](https://github.com/esowc/wildfire-forecasting/blob/master/src/test.py) which takes as argument a dict of configuration parameters and boolean for `benchmark`. You can check out more details about this function in [`test.py#L266`](https://github.com/esowc/wildfire-forecasting/blob/master/src/test.py#L266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.test import run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing the configuration options,\n",
    "\n",
    "```\n",
    "* in_days        -    no of input days of weather forcings\n",
    "* out_days       -    no of output days of fwi prediction\n",
    "* checkpoint_file-    path to pretrained checkpoint file\n",
    "* reanalysis_dir -    path to fwi-reanalysis data\n",
    "* forcings_dir   -    path to fwi-forcings data\n",
    "* forecast_dir   -    path to fwi-forecast data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    in_days=4,\n",
    "    out_days=10,\n",
    "    checkpoint_file=\"samplemodel/4_14/1ga5loax_59_60.ckpt\",\n",
    "    reanalysis_dir=\"sampledata/4_14/fwi-reanalysis\",\n",
    "    forcings_dir=\"sampledata/4_14/forcings\",\n",
    "    forecast_dir=\"sampledata/4_14/fwi-forecast\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We demonstrate below the metrics for deepFWI predicted values and benchmark them against the numerical fwi-forecast prediction values. Accuracy, Mean Squared Error (MSE) and Mean Absolute Error (MAE), being the best evaluation metrics for comparing regression models, these are presented for 1 FWI prediction (for 10 days) in a global setting. The metrics are calculated against fwi-reanalysis (considered the ground truth). The grouped bar plots indicate the metrics for deepFWI while the line plots are used for fwi-forecast. The FWI values are binned as per the EFFIS categorisation:\n",
    "\n",
    "```\n",
    "* [0, 5.2]     - Very Low\n",
    "* (5.2, 11.2]  - Low\n",
    "* (11.2, 21.3] - Moderate\n",
    "* (21.3, 38.0] - High\n",
    "* (38.0, 50.0] - Very High\n",
    "* (50.0, Inf)  - Extreme\n",
    "```\n",
    "\n",
    "As seen, the deepFWI predictions are consistent with FWI-Forecast and even offer better accuracy for longer term predictions in certain FWI categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with the model..\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Model:\n\tsize mismatch for conv.weight: copying a param with shape torch.Size([14, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([10, 16, 1, 1]).\n\tsize mismatch for conv.bias: copying a param with shape torch.Size([14]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7ef212381bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/wildfire-forecasting/src/test.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mbenchmark_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Doing inference with the model..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wildfire-forecasting/src/test.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(hparams, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/wildfire-dl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1045\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Model:\n\tsize mismatch for conv.weight: copying a param with shape torch.Size([14, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([10, 16, 1, 1]).\n\tsize mismatch for conv.bias: copying a param with shape torch.Size([14]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "run(**config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
